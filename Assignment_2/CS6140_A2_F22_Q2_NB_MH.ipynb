{"cells":[{"cell_type":"markdown","metadata":{"id":"0GFTkxHQUxC3"},"source":["# Q2 Naive-Bayes Classification (30 Points)\n","## Definition\n","Naive Bayes is a relatively simple classification algorithm based on probability and uses Bayes Theorom with an independence assumption among the features in the data. The fundamental idea of Naive Bayes is that it computes the probability of every class, which we want to reveal, based on the probability of every feature in the data.\n","\n","According to Naive Bayes algorithm, we are going to assume that every feature in the data is in an independent condition on the outcome probability of each separate class. Let's assume that we are doing a car classification and we have a data such as;\n","\n","| buying   | maint    | doors    | persons  | lug-boot | safety   | class    |\n","| :------- | :------- | :------- | :------- | :------- | :------- | :------- |\n","| vvhigh   | vhigh    | 2        | 2        | small    | low      | unacc    |\n","\n","**Description of dataset:**\n","* CAR                      car acceptability\n","    * PRICE                  overall price\n","        * _buying_               buying price\n","        * _maint_                price of the maintenance\n","* TECH                   technical characteristics\n","    * COMFORT              comfort\n","        * _doors_              number of doors\n","        * _persons_            capacity in terms of persons to carry\n","        * _lug-boot_           the size of luggage boot\n","    * _safety_               estimated safety of the car\n","   \n","Naive Bayes assumes that above mentioned features are independent of each other.\n","\n","In machine learning, Naive Bayes is advantageous against other commonly used classification algorithms because of its simplicity, speed and accuracy on small datasets and it also enables us to make classification despite missing information. Naive Bayes is a supervised learning algorithm because it needs to be trained with a labeled dataset."]},{"cell_type":"markdown","metadata":{"id":"ZdEqZEjdUxC5"},"source":["## Bayes Theorem\n","Consider two events, $A$ and $B$. For example, $A$ is a set of car features, which are $A \\in \\{ vvhigh, vhigh, 2, 2, small, low \\}$,and $B$ is a set of car classes that are $B \\in \\{ unacc, acc, good, vgood \\}$\n","\n","\n","* $A \\cap B$ means the intersection of $A$ and $B$.\n","* $P(A \\mid B)$ is read as probability of A given B.\n","\n","When we know that $B$ is given (Event $B$ has occurred), it means our sample space is $B$ that is the right figure. Now we are trying to compute the probability of also occuring $A$ at the same time (the conditional probability of $A$). It is obvious that we are trying to find the probability of $A \\cap B$ given that we are in the space of $B$.\n","\n","\\begin{equation}\n","P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\n","\\end{equation}\n","\n","We can rewrite $P(A \\cap B)$ as $P(A, B)$. Two of these mean the probability of $A$ and $B$ at the same time. So the new form of the equation is :\n","\n","\\begin{equation}\n","P(A \\mid B) = \\frac{P(A, B)}{P(B)}\n","\\end{equation}\n","\n","For the probability of $A$ and $B$, we can deduce equations below from the figure above.\n","\n","\\begin{align}\n","& P(A, B) = P(B, A) = P(A \\mid B)P(B) \\\\\n","& P(A, B) = P(B, A) = P(B \\mid A)P(A)\n","\\end{align}\n","\n","Let's look at the new form of the equation putting the second form of $P(A, B)$:\n","\n","\\begin{equation}\n","P(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B)}\n","\\end{equation}\n","\n","This equation is known as **Bayes Theorem**.\n","* $P(A \\mid B)$ : posterior that is the probability of $A$ when it is known that $B$ is given\n","* $P(B)$ : evidence that is the marginal probability of $B$\n","* $P(B \\mid A)$ : likelihood\n","* $P(A)$ : prior probability that is marginal probabiliy of $A$"]},{"cell_type":"markdown","metadata":{"id":"J2aZO4EYUxC6"},"source":["## Naive-Bayes Formulation\n","Suppose we have a dataset which each observation belongs to a class from the finite set $C = \\{ c_1, c_2, ..., c_n \\}$ and each observation constitutes from a few features $F = \\{ f_1, f_2, ..., f_b \\}$. If we could compute the probabilities of $P(c_1 | F), P(c_2 | F), ..., P(c_n | F)$ then we could predict the class for a new observation $i$ to be one of those which have the highest probability.\n","\n","To compute the conditional probabilities, we can use Bayes Theorem;\n","\n","\\begin{equation}\n","P(c_i \\mid f_1, f_2, \\dots ,f_b) = \\frac{P(f_1, f_2, \\dots ,f_b \\mid c_i)P(c_i)}{P(f_1, f_2, \\dots ,f_b)} \n","\\end{equation}\n","\n","As you know, Naive-Bayes supposes that all features are in independent conditions, therefore we can rewrite this equation like;\n","\n","\\begin{equation}\n","P(c_i \\mid f_1, f_2, \\dots ,f_b) = \\frac{P(f_1 \\mid c_i)P(f_2 \\mid c_i) \\dots P(f_b \\mid c_i)P(c_i)}{P(f_1, f_2, \\dots ,f_b)} \n","\\end{equation}\n","\n","The final form of equation is\n","\n","\\begin{align}\n","& \\text{for} \\; i = 1, 2, \\dots , n \\\\\n","& P(c_i \\mid f_1, f_2, \\dots ,f_b) = P(c_i) \\frac{\\Pi_{j=1}^b P(f_j \\mid c_i)}{P(f_1, f_2, \\dots ,f_b)} \n","\\end{align}\n","\n","Since $P(f_1, f_2, \\dots ,f_b)$ is a constant, we can use the classification rule below.\n","\n","\\begin{align}\n","& P(c_i \\mid f_1, f_2, \\dots ,f_b) \\propto P(c_i) \\Pi_{j=1}^b P(f_j \\mid c_i)\n","\\end{align}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-3d8iwGJle-K"},"source":["# Task. \n","- Use the 'car_eval.csv' data set. Train a Naive Bayes model  using odd-indexed rows. Test the accuracy of your model using even-indexed rows of the dataset.\n","- Display and explain the likelihood (Class conditional probabilities) for each input variable.\n","- Discuss one missclassified case for each category in terms of class conditional probabilities. Why do you think it was missclassified.\n","\n"]},{"cell_type":"code","execution_count":150,"metadata":{"id":"9S2PQvmcioOp"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accruacy = 70.95%\n"]}],"source":["from operator import pos\n","import pprint\n","import numpy as np\n","import pandas as pd\n","\n","\n","\n","# Returns a dictionary with the probabilities of each class\n","def getClassProbabilities(data):\n","    # Get the \"clazz\" column\n","    data = data.loc[:, \"clazz\"]\n","    output = dict(train_dataset.loc[:,\"clazz\"].value_counts(dropna=False, normalize=True))\n","    return output\n","\n","\n","# Returns a dictionary with the probabilities of each input variable\n","def getFeatureValueProbabilities(data):\n","    # exclude the \"clazz\" column\n","    data = data.iloc[:, :-1]\n","    columns = data.columns.tolist()\n","    output = {}\n","    for col in columns:\n","        output[col] = {}\n","        for key, val in dict(data[col].value_counts(dropna=False, normalize=True)).items():\n","            output[col][key] = val\n","        # for key, val in dict(data[col].value_counts(dropna=False)).items():\n","        #     output[col][key][\"count\"]: val\n","    return output\n","\n","\n","# Returns a dictionary with the joint probabilities of every input variable and class combination\n","def getJointProbabilities(data):\n","    op = data.loc[:, \"clazz\"]  # last column is the op\n","    ip = data.iloc[:, :6]  # first 6 columns are the ip\n","    output = {}\n","    # For each class in \"clazz\" column\n","\n","    for className, classProb in dict(op.value_counts(dropna=False, normalize=True)).items():\n","        # filter the data by the given class\n","        ip = data.loc[data[\"clazz\"] == className].iloc[:, :-1]\n","        # print(str(className) + \" length = \" + str(len(ip)))\n","        output[className] = {}\n","        # For each column in ip\n","        for ipCol in ip.columns.tolist():\n","            check = 0\n","            output[className][ipCol] = {}\n","            # Add all joint probabilities\n","            # For each value in ipCol\n","            for key, val in dict(ip[ipCol].value_counts(normalize=True)).items():\n","                p = val*classProb\n","                output[className][ipCol][key] = p\n","                check += p\n","    return output\n","\n","\n","# Calculate likelihoods for each input variable\n","def getLikelihoods(classProbabilities, jointProbabilities):\n","    output = {}\n","    # For each class\n","    for clazz in classProbabilities.keys():\n","        # Store P(class)\n","        pClass = classProbabilities[clazz]\n","        output[clazz] = {}\n","        # Iterate through every joint probability and multiply it by pClass, divide it by pFeature, then store in output\n","        for featureName in jointProbabilities[clazz].keys():\n","            output[clazz][featureName] = {}\n","            for featureValue in jointProbabilities[clazz][featureName].keys():\n","                jProb = jointProbabilities[clazz][featureName][featureValue]\n","                p = jProb / pClass \n","                # print(\"jProb of class \" + str(clazz))\n","                output[clazz][featureName][featureValue] = p\n","    return output\n","\n","\n","# Display the class conditional probabilities for each input variable\n","def printLikelihoods(likelihoods):\n","    for _class in likelihoods:\n","        print(\"class: \" + str(_class))\n","        for _colName in likelihoods[_class]:\n","            print(\"   \" + str(_colName))\n","            for _variable in likelihoods[_class][_colName]:\n","                prob = likelihoods[_class][_colName][_variable]\n","                print(\"      \" + str(_variable) + \": \" + str(round(prob, 4)))\n","\n","\n","# Calculate the posteriors - the probability of each class given a feature\n","def getPosteriors(classProbs, featureProbs, likelihoods):\n","    output = {}\n","    # For each class\n","    for clazz in classProbs.keys():\n","        # Store P(class)\n","        pClass = classProbs[clazz]\n","        output[clazz] = {}\n","        # Iterate through every joint probability and multiply it by pClass, divide it by pFeature, then store in output\n","        for featureName in likelihoods[clazz].keys():\n","            output[clazz][featureName] = {}\n","            for featureValue in likelihoods[clazz][featureName].keys():\n","                likelihood = likelihoods[clazz][featureName][featureValue]\n","                pFeature = featureProbs[featureName][featureValue]\n","                p = likelihood * pClass / pFeature\n","                output[clazz][featureName][featureValue] = p\n","    return output\n","    \n","\n","# Function to classify an instance by finding the class with the maximum product of posteriors for a given feature\n","def classifyInstance(testInstance, likelihoods, classProbs):\n","    # Ensure the testInstance doesn't include the \"class\" column\n","    data = testInstance.drop(index=[\"clazz\"])\n","    # Find the max likelihood\n","    max = 0\n","    prediction = None\n","    # For each class, get the product of it's class conditional probabilities across all features\n","    for className, classP in classProbs.items():\n","        likelihood_product = 1 # instantiate as 1\n","        # Find each class conditional probability (likelihood) and multiply them\n","        for featureName in data.keys().tolist():\n","            value = data[featureName]\n","            # Get likelihood\n","            if value in likelihoods[className][featureName].keys():\n","                likelihood = likelihoods[className][featureName][value]\n","            else:\n","                likelihood = 0\n","            # update the product of likelihoods\n","            likelihood_product *= likelihood\n","        # update prediction if the product of all class conditional probabilitiles > max\n","        if likelihood_product > max:\n","            prediction = className\n","            max = likelihood_product\n","    return prediction\n","\n","\n","# Function to classify all instances given a matrix/vector of test data\n","def classifyTestData(testData, posteriors, classProbs):\n","    output = testData.copy()\n","    output.reset_index()\n","    for index, row in output.iterrows():\n","        predictedClass = classifyInstance(row, posteriors, classProbs)\n","        output.at[index, \"clazz\"] = predictedClass\n","    return output\n","\n","\n","# Function to determine the accuracy of the model\n","def getModelAccuracy(actualData, predictions):\n","    total = 0\n","    right = 0\n","    for index, row in actualData.reset_index().iterrows():\n","        total += 1\n","        actual = actualData.iloc[index, -1]\n","        predicted = predictions.iloc[index, -1]\n","        if actual == predicted:\n","            right += 1\n","    return right / total\n","\n","\n","# Function to find and return a dataframe of misclassified instances\n","def getMisclassifiedInstances(predictions, actuals):\n","    columns = predictions.columns.tolist()\n","    columns.append(\"p_class\")\n","    p = predictions.to_numpy()\n","    a = actuals.to_numpy()\n","    a_ = a[p[:,-1]!=a[:,-1]]\n","    p_ = p[p[:,-1]!=a[:,-1], -1]\n","    z = np.insert(a_, -1, p_, axis=1)\n","    return pd.DataFrame(columns=columns, data=z)\n","    \n","\n","\n","# Read the data from .csv and split between test and training sets\n","dataset = pd.read_csv('car-eval.csv')\n","test_dataset = dataset[dataset.index % 2 == 0] # even indexes\n","train_dataset = dataset[dataset.index % 2 != 0] # odd indexes\n","test_dataset.reset_index()\n","train_dataset.reset_index()\n","\n","# Calculate probability of each class\n","classProbs = getClassProbabilities(train_dataset)\n","\n","# Calculate the probability of each feature value\n","featureProbs = getFeatureValueProbabilities(train_dataset)\n","\n","# Calculate joint probabilities\n","jointProbs = getJointProbabilities(train_dataset)\n","\n","# Calculate class conditional probabilities\n","likelihoods = getLikelihoods(classProbs, jointProbs)\n","\n","# Calculate postriors.... probabilities of a class given a feature\n","posteriors = getPosteriors(classProbs, featureProbs, likelihoods)\n","\n","# Make predictions\n","dataPredictions = classifyTestData(test_dataset, posteriors, classProbs)\n","\n","# Get misclassified instances\n","misClassified = getMisclassifiedInstances(dataPredictions, test_dataset)\n","\n","# Get model accuracy\n","modelAccuracy = getModelAccuracy(test_dataset, dataPredictions)\n","print(f\"Accruacy = {modelAccuracy*100:.2f}%\")\n","\n","\n"]},{"cell_type":"code","execution_count":146,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["class: unacc\n","   buying\n","      vhigh: 0.2982\n","      high: 0.2663\n","      med: 0.2211\n","      low: 0.2144\n","   maint\n","      vhigh: 0.2982\n","      high: 0.2596\n","      med: 0.2211\n","      low: 0.2211\n","   doors\n","      3: 0.258\n","      5more: 0.258\n","      2: 0.2529\n","      4: 0.2312\n","   persons\n","      2: 0.4824\n","      4: 0.2613\n","      more: 0.2563\n","   lug_boot\n","      small: 0.3702\n","      med: 0.3216\n","      big: 0.3082\n","   safety\n","      low: 0.4824\n","      med: 0.2965\n","      high: 0.2211\n","class: acc\n","   buying\n","      med: 0.298\n","      high: 0.2879\n","      low: 0.2222\n","      vhigh: 0.1919\n","   maint\n","      med: 0.298\n","      high: 0.2778\n","      low: 0.2323\n","      vhigh: 0.1919\n","   doors\n","      3: 0.2677\n","      4: 0.2576\n","      5more: 0.2576\n","      2: 0.2172\n","   persons\n","      4: 0.5\n","      more: 0.5\n","   lug_boot\n","      big: 0.3636\n","      med: 0.3586\n","      small: 0.2778\n","   safety\n","      high: 0.5455\n","      med: 0.4545\n","class: good\n","   buying\n","      low: 0.6667\n","      med: 0.3333\n","   maint\n","      low: 0.6667\n","      med: 0.3333\n","   doors\n","      3: 0.3077\n","      2: 0.2308\n","      4: 0.2308\n","      5more: 0.2308\n","   persons\n","      more: 0.5385\n","      4: 0.4615\n","   lug_boot\n","      med: 0.3846\n","      small: 0.3077\n","      big: 0.3077\n","   safety\n","      med: 0.5385\n","      high: 0.4615\n","class: vgood\n","   buying\n","      low: 0.6\n","      med: 0.4\n","   maint\n","      med: 0.4\n","      low: 0.4\n","      high: 0.2\n","   doors\n","      4: 0.3333\n","      5more: 0.3333\n","      2: 0.1667\n","      3: 0.1667\n","   persons\n","      4: 0.5\n","      more: 0.5\n","   lug_boot\n","      big: 0.6667\n","      med: 0.3333\n","   safety\n","      high: 1.0\n"]}],"source":["'''\n","The class conditional probabilities are printed below. Exampple of what they mean is:\n"," \"Given that the class equals \"unacc\", the probability of the 'Buying' feature being \"vhigh\" is 0.2982\n","'''\n","# print(classCondProbs)\n","printLikelihoods(likelihoods)"]},{"cell_type":"code","execution_count":148,"metadata":{"id":"vQh1UM6DWOy2"},"outputs":[{"name":"stdout","output_type":"stream","text":["For class unacc -> 251 examples were predicted incorrectly\n","For class acc -> 0 examples were predicted incorrectly\n","For class good -> 0 examples were predicted incorrectly\n","For class vgood -> 0 examples were predicted incorrectly\n","buying      vhigh\n","maint         med\n","doors           2\n","persons         4\n","lug_boot      med\n","safety       high\n","clazz       unacc\n","p_class       acc\n","Name: 0, dtype: object\n"]}],"source":["'''Discuss one misclassified case for each class and why it may have been misclassified:'''\n","\n","# For each class, print the total number of misclassified instances\n","def printNumMiscategorized(misclassified, classProbs):\n","    for category in classProbs.keys():\n","        # get list of misclassified examples for given category\n","        examples = misClassified[misClassified[\"clazz\"] == category]\n","        print(\"For class\", category, \"->\", len(examples), \"examples were predicted incorrectly\")\n","        # if (len(examples) > 0):\n","        #     print(examples.iloc[0, :])\n","\n","# For each class, print one example of a miscategorization\n","def printMiscategorizedExamples(misClassified, classProbs):\n","    for category in classProbs.keys():\n","        # printNumMiscategorized(misClassified, category)\n","        examples = misClassified[misClassified[\"clazz\"] == category]\n","        if (len(examples) > 0):\n","            print(examples.iloc[0, :])\n","\n","\n","printNumMiscategorized(misClassified, classProbs)\n","\n","printMiscategorizedExamples(misClassified, classProbs)\n","    \n"]},{"cell_type":"markdown","metadata":{},"source":["- All incorrect predictions were from class \"unacc\". In the test data, there are 613 instances of class \"unacc\" and 251 were predicted incorrectly based on their features.\n","- The input variable likelihoods for class \"unacc\" are close to being evenly distributed for a given feature. For example, take the class conditional probabilities of the \"buying\", \"maint\", and \"doors\" features below. All are roughly 1/4\n","    buying\n","      vhigh: 0.2982\n","      high: 0.2663\n","      med: 0.2211\n","      low: 0.2144\n","    maint\n","      vhigh: 0.2982\n","      high: 0.2596\n","      med: 0.2211\n","      low: 0.2211\n","    doors\n","      3: 0.258\n","      5more: 0.258\n","      2: 0.2529\n","      4: 0.2312\n","\n","Take the following example of a misclassified instance:\n","        buying      vhigh\n","        maint         med\n","        doors           2\n","        persons         4\n","        lug_boot      med\n","        safety       high\n","        clazz       unacc\n","        predicted_class       acc\n","This is misclassified because other classes have higher likelihoods for the given feature values:\n","    buying = vhigh:\n","      \"unacc\" = 0.2982\n","      \"acc\" = 0.1919\n","      \"good\" = 0\n","      \"vgood\" = 0    \n","    maint = med:\n","      \"unacc\" = 0.2211\n","      \"acc\" = 0.298\n","      \"good\" = 0.3333\n","      \"vgood\" = 0.4\n","    doors = 2\n","      \"unacc\" = 0.4824\n","      \"acc\" = 0.2172\n","      \"good\" = 0.2308\n","      \"vgood\" = 0.1667\n","    persons = 4\n","      \"unacc\" = 0.2613\n","      \"acc\" = 0.5\n","      \"good\" = 0.4615\n","      \"vgood\" = 0.5\n","    lug_boot = med\n","      \"unacc\" = 0.3216\n","      \"acc\" = 0.3586\n","      \"good\" = 0.3846\n","      \"vgood\" = 0.3333\n","    safety = high\n","      \"unacc\" = 0.2211\n","      \"acc\" = 0.5455\n","      \"good\" = 0.4615\n","      \"vgood\" = 1.0\n","    Result: Product of likelihoods across all features:\n","      \"unacc\" = 0.000590946\n","      \"acc\" = 0.00121486\n","      \"good\" = 0\n","      \"vgood\" = 0\n","\n","For the example above, class \"acc\" had the highest product of likelihoods and therefore the class was classified as \"acc\" (albeit incorrectly). "]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3.9.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"}}},"nbformat":4,"nbformat_minor":0}
